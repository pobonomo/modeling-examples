{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Subset Selection: L0-Regression\n",
    "\n",
    "## Objective and Prerequisites\n",
    "\n",
    "In this example, you will learn how to perform linear regression with feature selection using mathematical programming. We’ll show you how to construct a mixed-integer quadratic programming (MIQP) model of this linear regression problem, implement this model in the Gurobi Python API, and generate an optimal solution.\n",
    "\n",
    "This modeling example is at the intermediate level, where we assume that you know Python and are familiar with the Gurobi Python API. In addition, you should have some knowledge about building mathematical optimization models.\n",
    "\n",
    "**Download the Repository** <br />\n",
    "You can download the repository containing this and other examples by clicking [here](https://github.com/Gurobi/modeling-examples/archive/master.zip). \n",
    "\n",
    "**Gurobi License** <br />\n",
    "In order to run this Jupyter Notebook properly, you must have a Gurobi license. If you do not have one, you can request an [evaluation license](https://www.gurobi.com/downloads/request-an-evaluation-license/?utm_source=3PW&utm_medium=OT&utm_campaign=WW-MU-RAO-OR-O_LEA-PR_NO-Q3_FY20_WW_JPME_best-feature-selection-forecasting_COM_EVAL_GITHUB_&utm_term=feature-selection&utm_content=C_JPM) as a *commercial user*, or download a [free license](https://www.gurobi.com/academia/academic-program-and-licenses/?utm_source=3PW&utm_medium=OT&utm_campaign=WW-MU-RAO-OR-O_LEA-PR_NO-Q3_FY20_WW_JPME_best-feature-selection-forecasting_ACADEMIC_EVAL_GITHUB_&utm_term=feature-selection&utm_content=C_JPM) as an *academic user*.\n",
    "\n",
    "---\n",
    "## Motivation\n",
    "\n",
    "Linear regression was invented at the beginning of the 19th century and today, after more than 200 years, it is still used extensively in practical applications for description and prediction purposes:\n",
    "\n",
    "- In econometrics, it is useful to estimate the price elasticity of a particular product by regressing sales revenue on price and possibly other features such as demographics and competitor and retail information.\n",
    "- In health sciences, it can be applied to predict how long a patient will remain (i.e. length of stay) in the ER of a hospital based on patient information, triage assessment, medical test results, and the date/time of arrival.\n",
    "- In social sciences, it may shed light on future academic performance of students, so proactive measures can be taken to improve their learning outcomes.\n",
    "\n",
    "In general, linear regression is used to model the relationship between a continuous variable and other explanatory variables, which can be either continuous or categorical. When applying this technique, finding the subset of features that maximizes its perfomance is often of interest.\n",
    "\n",
    "---\n",
    "## Problem Description\n",
    "\n",
    "Linear regression is a supervised learning algorithm used to predict a quantitative response. It assumes that there is a linear relationship between the feature vector $x_i \\in \\mathbb{R}^d$ and the response $y_i \\in \\mathbb{R}$. Mathematically speaking, for sample $i$ we have $y_i = \\beta^T x_i + \\epsilon_i$, where $\\beta \\in \\mathbb{R}^d$ is the vector of feature weights, including the intercept, and  $\\epsilon_i$ is a normally-distributed random variable with zero mean and constant variance representing the error term. We can learn the weights from a training dataset with $n$ observations $\\{X \\in \\mathbb{M}^{nxd},y \\in \\mathbb{R}^n\\}$ by minimizing the Residual Sum of Squares (RSS): $e^Te =(y-X\\beta)^T (y-X\\beta)=\\beta^T X^T X\\beta- 2y^TX\\beta+y^T y$. The Ordinary Least Squares (OLS) method achieves this by taking the derivative of this quadratic and convex function and then finding the stationary point: $\\beta_{OLS}=(X^T X)^{-1} X^T y$.\n",
    "\n",
    "In practice, some of the features are in fact not associated with the response. By including them, we only add unnecessary complexity to the model and increase variance to the weight estimates. However, finding the best performing model is no simple task as there is an exponential number of candidates, as one has to test $\\sum_{s=1}^{d-1}{{d-1} \\choose s}$ models. Since OLS rarely yields estimates that are exactly zero, thus discarding the features related to them, we need to resort to feature selection methods. Popular methods include:\n",
    "\n",
    "- Subset selection, e.g. stepwise selection.\n",
    "- Dimensionality reduction, e.g. principal component regression.\n",
    "- Shrinkage, e.g. the Lasso.\n",
    "\n",
    "The Lasso has undoubtedly been the method of choice for the last decade. Basically it fits a model containing all $d$ predictors, while incorporating a budget constraint based on the L1-norm of $\\beta$, disregarding the intercept component. In fact, this method minimizes the RSS, subject to $\\sum_{l=1}^{d-1}\\mathopen|\\beta_l\\mathclose| \\leq s$, where $s$ is a hyper-parameter representing the budget that is usually tuned via cross-validation. This constraint has the effect of shrinking all weight estimates, allowing some of them to be exactly zero when $s$ is small enough. Finally, it is worth noting that the unconstrained version of the Lasso is more frequently used. This version solves an unconstrained optimization problem where $RSS + \\lambda \\sum_{l=1}^{d-1}\\mathopen|\\beta_l\\mathclose|$ is minimized, for a given value of the —modified— lagrangian multiplier $\\lambda \\in \\mathbb{R}^+$.  \n",
    "\n",
    "A similar formulation is now presented, where the L0-norm is used instead. We now seek to minimize the RSS, subject to $\\sum_{l=1}^{d-1}I(\\beta_l \\neq 0) \\leq s$, where $I(\\beta_l \\neq 0)$ is an indicator function taking on the value of 1 if $\\beta_j \\neq 0$ and 0 otherwise. In this setting, $s$ represents the number of features to consider in the model. This optimization problem may be casted as a Mixed Integer Quadratic Program (MIQP). Traditionally, the feature selection problem has not been tackled this way because of the common belief in the statistics community that large-scale problems are intractable. But this is no longer the case, considering the computing power currently available and the performance of modern optimization solvers such as Gurobi.\n",
    "\n",
    "---\n",
    "## Solution Approach\n",
    "\n",
    "Mathematical programming is a declarative approach where the modeler formulates a mathematical optimization model that captures the key aspects of a complex decision problem. The Gurobi Optimizer solves such models using state-of-the-art mathematics and computer science. \n",
    "\n",
    "A mathematical optimization model has five components, namely:\n",
    "\n",
    "- Sets and indices.\n",
    "- Parameters.\n",
    "- Decision variables.\n",
    "- Objective function(s).\n",
    "- Constraints.\n",
    "\n",
    "We now present a MIQP formulation that finds the weight estimates for a linear regression problem, where exactly $s$ of those weights can be nonzero:\n",
    "\n",
    "### Sets and Indices\n",
    "\n",
    "$i \\in I=\\{1,2,\\dots,n\\}$: Set of observations.\n",
    "\n",
    "$j \\in J=\\{1,2,\\dots,d\\}$: Set of features, where the last ID corresponds to the intercept.\n",
    "\n",
    "$l \\in L = J \\backslash \\{d\\}$: Set of features, where the intercept is excluded.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "$s \\in \\mathbb{N}$: Number of features to include in the model, ignoring the intercept.\n",
    "\n",
    "$Q = X^T X \\in \\mathbb{M}^{|J|x|J|}$: Quadratic component of the objective function.\n",
    "\n",
    "$c = y^TX \\in \\mathbb{R}^{|J|}$: Linear component of the objective function.\n",
    "\n",
    "$k = y^T y \\in \\mathbb{R}$: Constant component of the objective function.\n",
    "\n",
    "***Note:** Recall that the RSS is defined as $\\beta^T X^T X\\beta- 2y^TX\\beta+y^T y$.\n",
    "\n",
    "### Decision Variables\n",
    "\n",
    "$\\beta_j \\in \\mathbb{R}$: Weight of feature $j$, representing the change in the response variable per unit-change of feature $j$.\n",
    "\n",
    "$iszero_l \\in \\{0,1\\}$: 1 if weight of feature $l$ is exactly equal to zero, and 0 otherwise. Auxiliary variable used to manage the budget constraint.\n",
    "\n",
    "### Objective Function\n",
    "\n",
    "- **Training error**: Minimize the Residual Sum of Squares (RSS):\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Min} \\quad Z = \\frac{1}{2}\\sum_{j \\in J}\\sum_{j' \\in J}Q_{j,j'}\\beta_j\\beta_{j'} - \\sum_{j \\in J}c_j\\beta_j + \\frac{1}{2}k\n",
    "\\tag{0}\n",
    "\\end{equation}\n",
    "\n",
    "Note that we use the fact that if $x^*$ is a minimizer of $f(x)$, it is also a minimizer of $a\\cdot f(x)$, as long as $a > 0$.\n",
    "\n",
    "### Constraints\n",
    "\n",
    "- **Mutual exclusivity**: For each feature $l$, if $iszero_l=1$, then $\\beta_l=0$ (defined as a Special Ordered Set of type one):\n",
    "\n",
    "\\begin{equation}\n",
    "(\\beta_l, iszero_l): \\text{SOS-1} \\quad \\forall l \\in L\n",
    "\\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "- **Budget constraint**: Exactly $|L| - s$ feature weights must be equal to zero:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{l \\in L}iszero_l = |L| - s\n",
    "\\tag{2}\n",
    "\\end{equation}\n",
    "\n",
    "This model, by means of constraint 2, implicitly considers all ${{d-1} \\choose s}$ feature subsets at once. However, we also need to find the value for $s$ that maximizes the performance of the regression on unseen observations. Notice that the training RSS decreases monotonically as more features are considered (which translates to relaxing the MIQP), so it is not advisable to use it as the performance metric. Instead, we will estimate the Mean Squared Error (MSE) via cross-validation. This metric is defined as $\\text{MSE}=\\frac{1}{n}\\sum_{i=1}^{n}{(y_i-\\hat{y}_i)^2}$, where $y_i$ and $\\hat{y}_i$ are the observed and predicted values for the ith observation, respectively. Then, we will fine-tune $s$ using grid search, provided that the set of possible values is quite small.\n",
    "\n",
    "---\n",
    "## Python Implementation\n",
    "\n",
    "In the following implementation, we use four main libraries:\n",
    "\n",
    "- **Numpy** for scientific computing.\n",
    "- **Scikit learn** for machine learning algorithms.\n",
    "- **Gurobi** for mathematical optimization.\n",
    "- **Matplotlib** for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import all the necessary libraries\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and deploy the optimization model\n",
    "\n",
    "# NOTE: This function assumes the design matrix features does not contain\n",
    "#a column for the intercept\n",
    "def miqp(features, response, non_zero, verbose=False):\n",
    "    \"\"\"\n",
    "    Deploy and optimize the MIQP formulation of L0-Regression.\n",
    "    \"\"\"\n",
    "    assert isinstance(non_zero, (int, np.integer))\n",
    "    regressor = gp.Model()\n",
    "    samples, dim = features.shape\n",
    "    assert samples == response.shape[0]\n",
    "    assert non_zero <= dim\n",
    "\n",
    "    # Append a column of ones to the feature matrix to account for the y-intercept\n",
    "    X = np.concatenate([features, np.ones((samples, 1))], axis=1)  \n",
    "    \n",
    "    # Decision variables\n",
    "    beta = regressor.addMVar(dim + 1, lb=-GRB.INFINITY, name=\"beta\") # Weights\n",
    "    intercept = beta[dim] # Last decision variable captures the y-intercept\n",
    "    intercept.varname = 'intercept'\n",
    "    normvar = regressor.addVar(ub=non_zero, name='normvar')\n",
    "    \n",
    "    # Objective Function (OF): minimize 1/2 * RSS using the fact that\n",
    "    # if x* is a minimizer of f(x), it is also a minimizer of k*f(x) iff k > 0\n",
    "    Quad = np.dot(X.T, X)\n",
    "    lin = np.dot(response.T, X)\n",
    "    obj = 0.5 * (beta @ Quad @ beta)\n",
    "    obj -= lin @ beta\n",
    "    obj += 0.5 * np.dot(response, response)\n",
    "    regressor.setObjective(obj, GRB.MINIMIZE)\n",
    "    \n",
    "    # Constraint sets\n",
    "    regressor.addGenConstrNorm(normvar, beta[:dim], 0.0, \"normconstr\")\n",
    "    \n",
    "    if not verbose:\n",
    "        regressor.params.OutputFlag = 0\n",
    "    regressor.params.timelimit = 60\n",
    "    regressor.params.mipgap = 0.001\n",
    "    regressor.optimize()\n",
    "\n",
    "    coeff = beta.X[:dim]\n",
    "    return intercept.X, coeff     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions necessary to perform hyper-parameter tuning via cross-validation\n",
    "\n",
    "def split_folds(features, response, train_mask):\n",
    "    \"\"\"\n",
    "    Assign folds to either train or test partitions based on train_mask.\n",
    "    \"\"\"\n",
    "    xtrain = features[train_mask,:]\n",
    "    xtest = features[~train_mask,:]\n",
    "    ytrain = response[train_mask]\n",
    "    ytest = response[~train_mask]\n",
    "    return xtrain, xtest, ytrain, ytest\n",
    "\n",
    "def cross_validate(features, response, non_zero, folds, standardize, seed):\n",
    "    \"\"\"\n",
    "    Train an L0-Regression for each fold and report the cross-validated MSE.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    samples, dim = features.shape\n",
    "    assert samples == response.shape[0]\n",
    "    fold_size = int(np.ceil(samples / folds))\n",
    "    # Randomly assign each sample to a fold\n",
    "    shuffled = np.random.choice(samples, samples, replace=False)\n",
    "    mse_cv = 0\n",
    "    # Exclude folds from training, one at a time, \n",
    "    #to get out-of-sample estimates of the MSE\n",
    "    for fold in range(folds):\n",
    "        idx = shuffled[fold * fold_size : min((fold + 1) * fold_size, samples)]\n",
    "        train_mask = np.ones(samples, dtype=bool)\n",
    "        train_mask[idx] = False\n",
    "        xtrain, xtest, ytrain, ytest = split_folds(features, response, train_mask)\n",
    "        if standardize:\n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(xtrain)\n",
    "            xtrain = scaler.transform(xtrain)\n",
    "            xtest = scaler.transform(xtest)\n",
    "        intercept, beta = miqp(xtrain, ytrain, non_zero)\n",
    "        ypred = np.dot(xtest, beta) + intercept\n",
    "        mse_cv += mse(ytest, ypred) / folds\n",
    "    # Report the average out-of-sample MSE\n",
    "    return mse_cv\n",
    "\n",
    "def L0_regression(features, response, folds=5, standardize=False, seed=None):\n",
    "    \"\"\"\n",
    "    Select the best L0-Regression model by performing grid search on the budget.\n",
    "    \"\"\"\n",
    "    dim = features.shape[1]\n",
    "    best_mse = np.inf\n",
    "    best = 0\n",
    "    # Grid search to find best number of features to consider\n",
    "    for i in range(1, dim + 1):\n",
    "        val = cross_validate(features, response, i, folds=folds,\n",
    "                             standardize=standardize, seed=seed)\n",
    "        if val < best_mse:\n",
    "            best_mse = val\n",
    "            best = i\n",
    "    if standardize:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(features)\n",
    "        features = scaler.transform(features)\n",
    "    intercept, beta = miqp(features, response, best)\n",
    "    return intercept, beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Benchmark\n",
    "\n",
    "We now compare the performance of the aforementioned approach w.r.t. OLS regression on all features and the Lasso. The Boston dataset is used for this purpose. This dataset measures the prices of 506 houses, along with 13 features that provide insights about their neighbourhoods. We will use the original feature terminology, so the interested reader can visit [this website](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html) for more information.\n",
    "\n",
    "Note that 20% of the samples are reserved for computing the out-of-sample MSE. The resulting metrics are displayed in a bar chart (shown below) to facilitate the comparison between models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define how the bar chart should be displayed\n",
    "\n",
    "def plot_bar_chart(performance):\n",
    "    \"\"\"\n",
    "    Display the performance of all three models in a bar chart.\n",
    "    \"\"\"\n",
    "    bar = plt.bar([1, 2, 3], performance, color=['r', 'g', 'y'],\n",
    "                  tick_label=['OLS', 'Lasso', 'L0-Regression'])\n",
    "    plt.title('Out-of-Sample MSE')\n",
    "    x1, x2, y1, y2 = plt.axis()\n",
    "    plt.axis((x1, x2, np.floor(np.min(performance)),\n",
    "              np.ceil(np.max(performance))))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and split into train (80%) and test (20%)\n",
    "boston = load_boston()\n",
    "X = boston['data']\n",
    "y = boston['target']\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.20,\n",
    "                                                random_state=10101)\n",
    "\n",
    "# OLS regression using all features\n",
    "lr = linear_model.LinearRegression()\n",
    "lr.fit(Xtrain, ytrain)\n",
    "# Lasso with cross-validated penalization (lambda)\n",
    "lasso = linear_model.LassoCV(cv=5)\n",
    "lasso.fit(Xtrain, ytrain)\n",
    "# L0-regression where the best feature subset is selected via cross-validation\n",
    "intercept, beta = L0_regression(Xtrain, ytrain, seed=10101)\n",
    "\n",
    "# Compare their performance using a bar chart\n",
    "performance = []\n",
    "performance.append(mse(ytest, lr.predict(Xtest)))\n",
    "performance.append(mse(ytest, lasso.predict(Xtest)))\n",
    "performance.append(mse(ytest, np.dot(Xtest, beta) + intercept))\n",
    "plot_bar_chart(performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the Lasso performs poorly, as we did not standardize the features to be expressed in the same units (with an average of zero and variance of one). Contrary to OLS and L0-Regression, the Lasso is not scale-invariant because the budget constraint is based on the L1-norm. Remember that $\\beta_l$ is interpreted as the change in the response per unit-change of feature $l$. Since the L1-norm takes the sum of absolute values, how much of the budget $\\beta_l$ consumes depends on the units of measurement of the feature associated to it. \n",
    "\n",
    "Such preprocessing entails three steps, namely:\n",
    "\n",
    "For each feature $x_l$:\n",
    "1. Compute its sample average $\\mu_l$ and sample standard deviation $\\sigma_l$.\n",
    "2. Center by subtracting $\\mu_l$ from $x_l$.\n",
    "3. Scale by dividing the resulting difference by $\\sigma_l$.\n",
    "\n",
    "In order to report the performance of the Lasso after applying standardization, we need to perform hyper-parameter tuning on the L1-norm penalty via cross-validation. Unfortunately, we must not use the model class `LassoCV`. This is due to the fact that standardization is not supported, and doing that beforehand over the whole dataset would contaminate the folds. In order to prevent that from happening, we will perform random search as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10101)\n",
    "num_tries = 500\n",
    "best_alpha = None\n",
    "best_score = -np.inf\n",
    "for i in range(num_tries):\n",
    "    # log-linear search for alpha in the domain [0.001, 1000]\n",
    "    exponent = np.random.uniform(-3, 3)\n",
    "    alpha = np.power(10, exponent)\n",
    "    pipeline = make_pipeline(StandardScaler(), linear_model.Lasso(alpha=alpha))\n",
    "    scores = cross_val_score(pipeline, Xtrain, ytrain, cv=5, scoring='neg_mean_squared_error')\n",
    "    avg_score = np.mean(scores)\n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        best_alpha = alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now compare the performance of the models considered when the features are preprocessed. Notice that our user-defined function `L0-regression` does support standardization of the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features so they have an avg of 0 and a sample var of 1\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(Xtrain)\n",
    "Xtrain_std = scaler.transform(Xtrain)\n",
    "Xtest_std = scaler.transform(Xtest)\n",
    "\n",
    "# OLS regression using all features\n",
    "lr = linear_model.LinearRegression()\n",
    "lr.fit(Xtrain_std, ytrain)\n",
    "\n",
    "# Lasso with cross-validated penalization (lambda)\n",
    "lasso = linear_model.Lasso(alpha=best_alpha)\n",
    "lasso.fit(Xtrain_std, ytrain)\n",
    "# L0-regression where the best feature subset is selected via cross-validation\n",
    "intercept, beta = L0_regression(Xtrain, ytrain, standardize=True, seed=10101)\n",
    "\n",
    "# Compare their performance using a Bar chart\n",
    "performance = []\n",
    "performance.append(mse(ytest, lr.predict(Xtest_std)))\n",
    "performance.append(mse(ytest, lasso.predict(Xtest_std)))\n",
    "performance.append(mse(ytest, np.dot(Xtest_std, beta) + intercept))\n",
    "plot_bar_chart(performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the Lasso is better (although marginally) than OLS. This is due to the fact that the Lasso can retrieve the $\\beta_{OLS}$ estimate when the budget $s$ is big enough (alternatively, when $\\lambda$ is small enough). On the other hand, it is marginally worse than L0-Regression, mainly because by shrinking $\\beta$ we add bias to the estimates. Furthermore, observe that L0-Regression achieved the best performance with the fewest number of features. This is convenient, as it leads to a more interpretable model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_features = np.sum(np.abs(lr.coef_) >= 1e-8)\n",
    "lasso_features = np.sum(np.abs(lasso.coef_) >= 1e-8)\n",
    "l0_features = np.sum(np.abs(beta) >= 1e-8)\n",
    "print(\"OLS regression kept {0} features.\".format(ols_features))\n",
    "print(\"The Lasso kept {0} features.\".format(lasso_features))\n",
    "print(\"L0-Regression kept {0} features.\".format(l0_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model\n",
    "\n",
    "The previous analysis indicates that the best candidate is the model suggested by L0-Regression. The resulting equation is as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{medv} = 22.56-1.02\\text{crim}+1.46\\text{zn}+0.49\\text{chas}-1.93\\text{nox}+2.53\\text{rm}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "-3.48\\text{dis}+2.65\\text{rad}-2.22\\text{tax}-1.87\\text{ptratio}+1.00\\text{b}-3.69\\text{lstat}\n",
    "\\end{equation}\n",
    "\n",
    "**Note:** The mean and variance vectors used in the standardization step can be accessed through `scaler.mean_` and `scaler.var_`, respectively. \n",
    "\n",
    "Since we standardized the data, the intercept represents the estimated median value (in thousands) of a house with mean values across features. Likewise, we can interpret $\\beta_1=-1.02$ as the decrease in the house value when the per-capita crime rate increases by one standard deviation from the average value, all other things being equal (similar statements can be made for the rest of the features). Finally, if the main purpose of the analysis is to explain the variability in the response, having 11 features may be too much. However, remember that one can always set the number of active features to a more manageable number to ease the interpretation, perhaps at the expense of predictive power.\n",
    "\n",
    "---\n",
    "## Conclusions\n",
    "\n",
    "It has been shown how mathematical programming can be used to perform feature selection on linear regression problems. It is in fact a good alternative to the Lasso, given that L0-Regression is scale invariant and does not add bias to the weight estimates. Furthermore, this approach is amenable to the specification of additional linear constraints (Bertsimas, 2015), such as:\n",
    "\n",
    "- Enforcing group sparsity among features.\n",
    "- Limiting pairwise multicollinearity.\n",
    "- Limiting global multicollinearity.\n",
    "- Considering a fixed set of nonlinear transformations.\n",
    "\n",
    "Nevertheless, take this result with caution, as \"there is no free lunch in statistics\". That is, no algorithm outperforms all others under all possible datasets. Ultimately, a good data scientist should consider multiple learning algorithms when analyzing a dataset.\n",
    "\n",
    "---\n",
    "## References\n",
    "\n",
    "1. Bertsimas, D., & King, A. (2015). OR forum—An algorithmic approach to linear regression. Operations Research, 64(1), 2-16.\n",
    "2. Bertsimas, D., King, A., & Mazumder, R. (2016). Best subset selection via a modern optimization lens. The annals of statistics, 44(2), 813-852.\n",
    "3. James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An introduction to statistical learning. New York: springer.\n",
    "4. The Boston housing dataset (1996, October 10). Retrieved from https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © 2020 Gurobi Optimization, LLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
